{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lesson8_17_char_Lev_Rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolopdel/Lesson_8_17_char_RNN_songs/blob/master/Lesson8_17_char_Lev_Rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZlEJW8CYKxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TydXojjSYKyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('data/Output1.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_brwIpBXYKy0",
        "colab_type": "code",
        "outputId": "806cdc38-9bfa-45ea-a6d6-9dc4f05be855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "text[:100]\n",
        "#len(text)\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ufeffDoesn't take much to make me happy\\nAnd make me smile with glee \\nNever never will I feel discouraged\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyGAcmYoYKzT",
        "colab_type": "code",
        "outputId": "c8a8ec8a-da31-494c-accc-11f92dc24e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "print(chars)\n",
        "int2char = dict(enumerate(chars))\n",
        "print(int2char)\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "print(char2int)\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "len(chars)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('m', '”', 'é', \"'\", 'w', 'c', '-', '&', '値', 'S', 'D', 'U', 'f', 'j', '1', '[', 'H', 'n', '\"', '“', 'C', 'W', 'B', 'O', 'u', 'ì', 's', '4', 'x', '—', 'y', 'V', '’', '7', 'r', '}', '`', '\\n', 'k', ')', 'M', 'q', '貓', 'K', 'i', ' ', '.', 'h', '_', 'R', 'è', 'F', '…', ';', 'd', 'P', 'Q', 'o', '#', 'Y', ',', 't', 'È', '‘', '2', '\\\\', '\\ufeff', '8', 'à', '3', '0', 'z', 'b', 'p', 'v', 'a', '!', 'I', '(', '{', 'l', ':', 'ù', '5', '´', 'T', 'E', '9', '/', 'ç', ']', '\\t', '?', '6', 'e', '~', 'A', 'Z', 'ã', 'G', 'J', 'g', 'N', '王', 'L')\n",
            "{0: 'm', 1: '”', 2: 'é', 3: \"'\", 4: 'w', 5: 'c', 6: '-', 7: '&', 8: '値', 9: 'S', 10: 'D', 11: 'U', 12: 'f', 13: 'j', 14: '1', 15: '[', 16: 'H', 17: 'n', 18: '\"', 19: '“', 20: 'C', 21: 'W', 22: 'B', 23: 'O', 24: 'u', 25: 'ì', 26: 's', 27: '4', 28: 'x', 29: '—', 30: 'y', 31: 'V', 32: '’', 33: '7', 34: 'r', 35: '}', 36: '`', 37: '\\n', 38: 'k', 39: ')', 40: 'M', 41: 'q', 42: '貓', 43: 'K', 44: 'i', 45: ' ', 46: '.', 47: 'h', 48: '_', 49: 'R', 50: 'è', 51: 'F', 52: '…', 53: ';', 54: 'd', 55: 'P', 56: 'Q', 57: 'o', 58: '#', 59: 'Y', 60: ',', 61: 't', 62: 'È', 63: '‘', 64: '2', 65: '\\\\', 66: '\\ufeff', 67: '8', 68: 'à', 69: '3', 70: '0', 71: 'z', 72: 'b', 73: 'p', 74: 'v', 75: 'a', 76: '!', 77: 'I', 78: '(', 79: '{', 80: 'l', 81: ':', 82: 'ù', 83: '5', 84: '´', 85: 'T', 86: 'E', 87: '9', 88: '/', 89: 'ç', 90: ']', 91: '\\t', 92: '?', 93: '6', 94: 'e', 95: '~', 96: 'A', 97: 'Z', 98: 'ã', 99: 'G', 100: 'J', 101: 'g', 102: 'N', 103: '王', 104: 'L'}\n",
            "{'m': 0, '”': 1, 'é': 2, \"'\": 3, 'w': 4, 'c': 5, '-': 6, '&': 7, '値': 8, 'S': 9, 'D': 10, 'U': 11, 'f': 12, 'j': 13, '1': 14, '[': 15, 'H': 16, 'n': 17, '\"': 18, '“': 19, 'C': 20, 'W': 21, 'B': 22, 'O': 23, 'u': 24, 'ì': 25, 's': 26, '4': 27, 'x': 28, '—': 29, 'y': 30, 'V': 31, '’': 32, '7': 33, 'r': 34, '}': 35, '`': 36, '\\n': 37, 'k': 38, ')': 39, 'M': 40, 'q': 41, '貓': 42, 'K': 43, 'i': 44, ' ': 45, '.': 46, 'h': 47, '_': 48, 'R': 49, 'è': 50, 'F': 51, '…': 52, ';': 53, 'd': 54, 'P': 55, 'Q': 56, 'o': 57, '#': 58, 'Y': 59, ',': 60, 't': 61, 'È': 62, '‘': 63, '2': 64, '\\\\': 65, '\\ufeff': 66, '8': 67, 'à': 68, '3': 69, '0': 70, 'z': 71, 'b': 72, 'p': 73, 'v': 74, 'a': 75, '!': 76, 'I': 77, '(': 78, '{': 79, 'l': 80, ':': 81, 'ù': 82, '5': 83, '´': 84, 'T': 85, 'E': 86, '9': 87, '/': 88, 'ç': 89, ']': 90, '\\t': 91, '?': 92, '6': 93, 'e': 94, '~': 95, 'A': 96, 'Z': 97, 'ã': 98, 'G': 99, 'J': 100, 'g': 101, 'N': 102, '王': 103, 'L': 104}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-5srjNSYKzs",
        "colab_type": "code",
        "outputId": "e4cad3e7-b3df-40ba-e406-aaa76e9ad30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66,  10,  57,  94,  26,  17,   3,  61,  45,  61,  75,  38,  94,\n",
              "        45,   0,  24,   5,  47,  45,  61,  57,  45,   0,  75,  38,  94,\n",
              "        45,   0,  94,  45,  47,  75,  73,  73,  30,  37,  96,  17,  54,\n",
              "        45,   0,  75,  38,  94,  45,   0,  94,  45,  26,   0,  44,  80,\n",
              "        94,  45,   4,  44,  61,  47,  45, 101,  80,  94,  94,  45,  37,\n",
              "       102,  94,  74,  94,  34,  45,  17,  94,  74,  94,  34,  45,   4,\n",
              "        44,  80,  80,  45,  77,  45,  12,  94,  94,  80,  45,  54,  44,\n",
              "        26,   5,  57,  24,  34,  75, 101,  94,  54])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu3czHiMYK0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "\n",
        "#crea un array: 3 con aggiuna 8 (3,8)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7JVgh_2YK0a",
        "colab_type": "code",
        "outputId": "d7bc05ca-e561-4ce5-9dd8-a9771cf776f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)\n",
        "#print(test_seq)   #[[3 5 1]]\n",
        "#print(test_seq.shape)#(1, 3)\n",
        "#print(test_seq.size)   #3\n",
        "#print(test_seq.flatten())  #[3 5 1]\n",
        "#print(test_seq) #[[3 5 1]]\n",
        "#arange che si trova nel codice sopra è come for, ma parte da dove si vuole, primo argomento, passo nel secondo arg.\n",
        "#l'array 3,5,1 viene posto come primo arg. "
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uunXKesPYK0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#starting sequence: [1 2 3 4 5 6 7 8 9 10 11 12]\n",
        "#Batch size: 2   [1 2 3 4 5 6 ]\n",
        "#               [7 8 9 10 11 12]\n",
        "#sequence lenght: 3  [1 2 3 ......\n",
        "#                    [7 8 9 ......\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aPfEkQ7YK1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umkGDK-hYK1d",
        "colab_type": "code",
        "outputId": "495ce88c-4041-4eea-dfc7-88e501086979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "print(batches)\n",
        "1985223//400"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object get_batches at 0x7fa2e72cc1a8>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwZTPCTeYK11",
        "colab_type": "code",
        "outputId": "1017ddbd-a1fc-4e2c-879d-a4f9e3483aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 66  10  57  94  26  17   3  61  45  61]\n",
            " [ 24  45  75  34  94  45   0  30  45  80]\n",
            " [ 61  47  94  45  72  34  94  94  71  94]\n",
            " [ 74  94  34  45  80  94  61  45  30  57]\n",
            " [ 45  61  94  80  80  45  30  57  24  45]\n",
            " [ 57  24  61  45  30  57  24  37  37  99]\n",
            " [101  44  34  80  45  80  57  57  38  45]\n",
            " [101  37  37   9  75  30  45   0  30  45]]\n",
            "\n",
            "y\n",
            " [[10 57 94 26 17  3 61 45 61 75]\n",
            " [45 75 34 94 45  0 30 45 80 75]\n",
            " [47 94 45 72 34 94 94 71 94 26]\n",
            " [94 34 45 80 94 61 45 30 57 24]\n",
            " [61 94 80 80 45 30 57 24 45 75]\n",
            " [24 61 45 30 57 24 37 37 99 44]\n",
            " [44 34 80 45 80 57 57 38 45 75]\n",
            " [37 37  9 75 30 45  0 30 45 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03y96FN4YK2N",
        "colab_type": "code",
        "outputId": "6397c8d6-cad6-4ae9-93cd-728b15a48e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksX68WAYK2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b7CeXISYK2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            #h = tuple([each.data for each in h])\n",
        "            h = tuple([each.data for each in h])\n",
        "            #print(h)\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                   \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFgwjh7CYK2_",
        "colab_type": "code",
        "outputId": "e2c8a8a2-c62e-488d-b9fe-0e7c90e4b78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net.chars)\n",
        "len(net.chars)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('m', '”', 'é', \"'\", 'w', 'c', '-', '&', '値', 'S', 'D', 'U', 'f', 'j', '1', '[', 'H', 'n', '\"', '“', 'C', 'W', 'B', 'O', 'u', 'ì', 's', '4', 'x', '—', 'y', 'V', '’', '7', 'r', '}', '`', '\\n', 'k', ')', 'M', 'q', '貓', 'K', 'i', ' ', '.', 'h', '_', 'R', 'è', 'F', '…', ';', 'd', 'P', 'Q', 'o', '#', 'Y', ',', 't', 'È', '‘', '2', '\\\\', '\\ufeff', '8', 'à', '3', '0', 'z', 'b', 'p', 'v', 'a', '!', 'I', '(', '{', 'l', ':', 'ù', '5', '´', 'T', 'E', '9', '/', 'ç', ']', '\\t', '?', '6', 'e', '~', 'A', 'Z', 'ã', 'G', 'J', 'g', 'N', '王', 'L')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyCWY27_YK3P",
        "colab_type": "code",
        "outputId": "a4886f06-847c-4bd4-ed47-706b9707e494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#batch_size = 128\n",
        "#seq_length = 100\n",
        "#n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior lr=0.001\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.3030... Val Loss: 3.2974\n",
            "Epoch: 1/20... Step: 20... Loss: 3.2211... Val Loss: 3.2124\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1805... Val Loss: 3.2064\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1665... Val Loss: 3.2025\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1370... Val Loss: 3.2034\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1551... Val Loss: 3.2020\n",
            "Epoch: 2/20... Step: 70... Loss: 3.1512... Val Loss: 3.1985\n",
            "Epoch: 2/20... Step: 80... Loss: 3.1428... Val Loss: 3.1958\n",
            "Epoch: 2/20... Step: 90... Loss: 3.1300... Val Loss: 3.1918\n",
            "Epoch: 2/20... Step: 100... Loss: 3.1197... Val Loss: 3.1814\n",
            "Epoch: 2/20... Step: 110... Loss: 3.0938... Val Loss: 3.1499\n",
            "Epoch: 2/20... Step: 120... Loss: 3.0261... Val Loss: 3.0814\n",
            "Epoch: 3/20... Step: 130... Loss: 2.9963... Val Loss: 3.0358\n",
            "Epoch: 3/20... Step: 140... Loss: 2.8764... Val Loss: 2.9094\n",
            "Epoch: 3/20... Step: 150... Loss: 2.7594... Val Loss: 2.8154\n",
            "Epoch: 3/20... Step: 160... Loss: 2.6530... Val Loss: 2.7242\n",
            "Epoch: 3/20... Step: 170... Loss: 2.5476... Val Loss: 2.6458\n",
            "Epoch: 3/20... Step: 180... Loss: 2.4555... Val Loss: 2.5780\n",
            "Epoch: 4/20... Step: 190... Loss: 2.4560... Val Loss: 2.5416\n",
            "Epoch: 4/20... Step: 200... Loss: 2.3841... Val Loss: 2.4882\n",
            "Epoch: 4/20... Step: 210... Loss: 2.3401... Val Loss: 2.4543\n",
            "Epoch: 4/20... Step: 220... Loss: 2.2304... Val Loss: 2.4215\n",
            "Epoch: 4/20... Step: 230... Loss: 2.2516... Val Loss: 2.3971\n",
            "Epoch: 4/20... Step: 240... Loss: 2.1707... Val Loss: 2.3724\n",
            "Epoch: 4/20... Step: 250... Loss: 2.1803... Val Loss: 2.3454\n",
            "Epoch: 5/20... Step: 260... Loss: 2.1352... Val Loss: 2.3230\n",
            "Epoch: 5/20... Step: 270... Loss: 2.1282... Val Loss: 2.2895\n",
            "Epoch: 5/20... Step: 280... Loss: 2.1163... Val Loss: 2.2737\n",
            "Epoch: 5/20... Step: 290... Loss: 2.1061... Val Loss: 2.2411\n",
            "Epoch: 5/20... Step: 300... Loss: 2.0416... Val Loss: 2.2232\n",
            "Epoch: 5/20... Step: 310... Loss: 2.0134... Val Loss: 2.2057\n",
            "Epoch: 6/20... Step: 320... Loss: 2.0014... Val Loss: 2.1814\n",
            "Epoch: 6/20... Step: 330... Loss: 1.9847... Val Loss: 2.1545\n",
            "Epoch: 6/20... Step: 340... Loss: 1.9517... Val Loss: 2.1380\n",
            "Epoch: 6/20... Step: 350... Loss: 1.9620... Val Loss: 2.1178\n",
            "Epoch: 6/20... Step: 360... Loss: 1.8981... Val Loss: 2.1079\n",
            "Epoch: 6/20... Step: 370... Loss: 1.8826... Val Loss: 2.1011\n",
            "Epoch: 7/20... Step: 380... Loss: 1.9211... Val Loss: 2.0770\n",
            "Epoch: 7/20... Step: 390... Loss: 1.8722... Val Loss: 2.0530\n",
            "Epoch: 7/20... Step: 400... Loss: 1.8154... Val Loss: 2.0385\n",
            "Epoch: 7/20... Step: 410... Loss: 1.8162... Val Loss: 2.0266\n",
            "Epoch: 7/20... Step: 420... Loss: 1.8354... Val Loss: 2.0107\n",
            "Epoch: 7/20... Step: 430... Loss: 1.8099... Val Loss: 2.0014\n",
            "Epoch: 7/20... Step: 440... Loss: 1.7732... Val Loss: 1.9882\n",
            "Epoch: 8/20... Step: 450... Loss: 1.7859... Val Loss: 1.9765\n",
            "Epoch: 8/20... Step: 460... Loss: 1.7568... Val Loss: 1.9642\n",
            "Epoch: 8/20... Step: 470... Loss: 1.7395... Val Loss: 1.9515\n",
            "Epoch: 8/20... Step: 480... Loss: 1.7112... Val Loss: 1.9420\n",
            "Epoch: 8/20... Step: 490... Loss: 1.7147... Val Loss: 1.9341\n",
            "Epoch: 8/20... Step: 500... Loss: 1.7470... Val Loss: 1.9232\n",
            "Epoch: 9/20... Step: 510... Loss: 1.6885... Val Loss: 1.9174\n",
            "Epoch: 9/20... Step: 520... Loss: 1.6880... Val Loss: 1.9078\n",
            "Epoch: 9/20... Step: 530... Loss: 1.6546... Val Loss: 1.8944\n",
            "Epoch: 9/20... Step: 540... Loss: 1.6685... Val Loss: 1.8888\n",
            "Epoch: 9/20... Step: 550... Loss: 1.6497... Val Loss: 1.8868\n",
            "Epoch: 9/20... Step: 560... Loss: 1.6483... Val Loss: 1.8827\n",
            "Epoch: 10/20... Step: 570... Loss: 1.6777... Val Loss: 1.8701\n",
            "Epoch: 10/20... Step: 580... Loss: 1.6444... Val Loss: 1.8564\n",
            "Epoch: 10/20... Step: 590... Loss: 1.6506... Val Loss: 1.8492\n",
            "Epoch: 10/20... Step: 600... Loss: 1.5868... Val Loss: 1.8442\n",
            "Epoch: 10/20... Step: 610... Loss: 1.6223... Val Loss: 1.8361\n",
            "Epoch: 10/20... Step: 620... Loss: 1.6126... Val Loss: 1.8292\n",
            "Epoch: 10/20... Step: 630... Loss: 1.6216... Val Loss: 1.8205\n",
            "Epoch: 11/20... Step: 640... Loss: 1.6369... Val Loss: 1.8164\n",
            "Epoch: 11/20... Step: 650... Loss: 1.5821... Val Loss: 1.8065\n",
            "Epoch: 11/20... Step: 660... Loss: 1.5496... Val Loss: 1.8038\n",
            "Epoch: 11/20... Step: 670... Loss: 1.5955... Val Loss: 1.8068\n",
            "Epoch: 11/20... Step: 680... Loss: 1.5662... Val Loss: 1.7934\n",
            "Epoch: 11/20... Step: 690... Loss: 1.5814... Val Loss: 1.7847\n",
            "Epoch: 12/20... Step: 700... Loss: 1.5756... Val Loss: 1.7834\n",
            "Epoch: 12/20... Step: 710... Loss: 1.5276... Val Loss: 1.7776\n",
            "Epoch: 12/20... Step: 720... Loss: 1.5262... Val Loss: 1.7740\n",
            "Epoch: 12/20... Step: 730... Loss: 1.5240... Val Loss: 1.7642\n",
            "Epoch: 12/20... Step: 740... Loss: 1.5573... Val Loss: 1.7656\n",
            "Epoch: 12/20... Step: 750... Loss: 1.5247... Val Loss: 1.7629\n",
            "Epoch: 13/20... Step: 760... Loss: 1.5533... Val Loss: 1.7573\n",
            "Epoch: 13/20... Step: 770... Loss: 1.5236... Val Loss: 1.7391\n",
            "Epoch: 13/20... Step: 780... Loss: 1.5115... Val Loss: 1.7463\n",
            "Epoch: 13/20... Step: 790... Loss: 1.4836... Val Loss: 1.7458\n",
            "Epoch: 13/20... Step: 800... Loss: 1.4862... Val Loss: 1.7364\n",
            "Epoch: 13/20... Step: 810... Loss: 1.4896... Val Loss: 1.7342\n",
            "Epoch: 14/20... Step: 820... Loss: 1.5395... Val Loss: 1.7238\n",
            "Epoch: 14/20... Step: 830... Loss: 1.4947... Val Loss: 1.7142\n",
            "Epoch: 14/20... Step: 840... Loss: 1.4608... Val Loss: 1.7154\n",
            "Epoch: 14/20... Step: 850... Loss: 1.4526... Val Loss: 1.7108\n",
            "Epoch: 14/20... Step: 860... Loss: 1.4712... Val Loss: 1.7145\n",
            "Epoch: 14/20... Step: 870... Loss: 1.4364... Val Loss: 1.7064\n",
            "Epoch: 14/20... Step: 880... Loss: 1.4698... Val Loss: 1.6987\n",
            "Epoch: 15/20... Step: 890... Loss: 1.4579... Val Loss: 1.7096\n",
            "Epoch: 15/20... Step: 900... Loss: 1.3991... Val Loss: 1.7056\n",
            "Epoch: 15/20... Step: 910... Loss: 1.4589... Val Loss: 1.6942\n",
            "Epoch: 15/20... Step: 920... Loss: 1.4346... Val Loss: 1.6914\n",
            "Epoch: 15/20... Step: 930... Loss: 1.4460... Val Loss: 1.6926\n",
            "Epoch: 15/20... Step: 940... Loss: 1.4221... Val Loss: 1.6841\n",
            "Epoch: 16/20... Step: 950... Loss: 1.4153... Val Loss: 1.6751\n",
            "Epoch: 16/20... Step: 960... Loss: 1.4340... Val Loss: 1.6721\n",
            "Epoch: 16/20... Step: 970... Loss: 1.4071... Val Loss: 1.6731\n",
            "Epoch: 16/20... Step: 980... Loss: 1.4257... Val Loss: 1.6691\n",
            "Epoch: 16/20... Step: 990... Loss: 1.3993... Val Loss: 1.6725\n",
            "Epoch: 16/20... Step: 1000... Loss: 1.3831... Val Loss: 1.6763\n",
            "Epoch: 17/20... Step: 1010... Loss: 1.4430... Val Loss: 1.6665\n",
            "Epoch: 17/20... Step: 1020... Loss: 1.4009... Val Loss: 1.6620\n",
            "Epoch: 17/20... Step: 1030... Loss: 1.3712... Val Loss: 1.6613\n",
            "Epoch: 17/20... Step: 1040... Loss: 1.3917... Val Loss: 1.6558\n",
            "Epoch: 17/20... Step: 1050... Loss: 1.4020... Val Loss: 1.6558\n",
            "Epoch: 17/20... Step: 1060... Loss: 1.3723... Val Loss: 1.6459\n",
            "Epoch: 17/20... Step: 1070... Loss: 1.3660... Val Loss: 1.6520\n",
            "Epoch: 18/20... Step: 1080... Loss: 1.3805... Val Loss: 1.6460\n",
            "Epoch: 18/20... Step: 1090... Loss: 1.3480... Val Loss: 1.6451\n",
            "Epoch: 18/20... Step: 1100... Loss: 1.3665... Val Loss: 1.6406\n",
            "Epoch: 18/20... Step: 1110... Loss: 1.3344... Val Loss: 1.6416\n",
            "Epoch: 18/20... Step: 1120... Loss: 1.3645... Val Loss: 1.6403\n",
            "Epoch: 18/20... Step: 1130... Loss: 1.3898... Val Loss: 1.6305\n",
            "Epoch: 19/20... Step: 1140... Loss: 1.3249... Val Loss: 1.6372\n",
            "Epoch: 19/20... Step: 1150... Loss: 1.3443... Val Loss: 1.6328\n",
            "Epoch: 19/20... Step: 1160... Loss: 1.3141... Val Loss: 1.6309\n",
            "Epoch: 19/20... Step: 1170... Loss: 1.3372... Val Loss: 1.6269\n",
            "Epoch: 19/20... Step: 1180... Loss: 1.3154... Val Loss: 1.6370\n",
            "Epoch: 19/20... Step: 1190... Loss: 1.3215... Val Loss: 1.6325\n",
            "Epoch: 20/20... Step: 1200... Loss: 1.3752... Val Loss: 1.6247\n",
            "Epoch: 20/20... Step: 1210... Loss: 1.3212... Val Loss: 1.6299\n",
            "Epoch: 20/20... Step: 1220... Loss: 1.3217... Val Loss: 1.6247\n",
            "Epoch: 20/20... Step: 1230... Loss: 1.2938... Val Loss: 1.6200\n",
            "Epoch: 20/20... Step: 1240... Loss: 1.3225... Val Loss: 1.6193\n",
            "Epoch: 20/20... Step: 1250... Loss: 1.3283... Val Loss: 1.6152\n",
            "Epoch: 20/20... Step: 1260... Loss: 1.3315... Val Loss: 1.6194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd3SX7ZzYK3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRkAIm3uYK3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        #print(x)\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        #print(x)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            #print(top_ch)\n",
        "            #print(p.topk(top_k))\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "            #print(top_ch)\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        #print(char)\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTbmautnYK3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Milano sembra Rio', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2hNF2O8YK4T",
        "colab_type": "code",
        "outputId": "0019a253-9c24-41c7-a874-f19a890ce466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(sample(net, 5000, prime='Milano sembra Rio', top_k=5))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Milano sembra Riother sterness\n",
            "We couldn't make it to me \n",
            "You are the thought oh, time, you're my lover\n",
            "\n",
            "You know you'll live, you and me all you're makes\n",
            "You're to me the morning our heart,, I never been thinking for a little dire\n",
            "She dail\n",
            "I don't'll be your\n",
            "I'll be lowe to yee\n",
            "\n",
            "If you can't give me who waiting for all my life\n",
            "I don't wanna believe in things\n",
            "And the way you see it what you're seeming means \n",
            "I'm the one that I want my heart a love find\n",
            "I've got to love you with me in my shile\n",
            "It was love\n",
            "\n",
            "The world and going to terstit me and then I love you all\n",
            "I'm tilling you are the one that I could never be my life\n",
            "\n",
            "I've been there with you \n",
            "And I've been strend a wonlore \n",
            "To the sand to too so but of all I can't do\n",
            "\n",
            "And that I walk a morning\n",
            "And I'm stoply for love \n",
            "It's and love if you're always like you \n",
            "\n",
            "You're said, I can't wait for lovin' you, baby\n",
            "And I want you're that I love you\n",
            "And I, I won't love you so beat\n",
            "\n",
            "All the way you see the darking of your eyes\n",
            "You are the way that I've stand for the light, \n",
            "This is the let you get my say\n",
            "It's stepling from that was wain to mine\n",
            "I need the way\n",
            "I've got to say that you've been wanting forever\n",
            "The one that\n",
            "I need my searing for me\n",
            "\n",
            "Tell me have all your love\n",
            "\n",
            "If it wish me\n",
            "\n",
            "You wort out my love, lake an love\n",
            "\n",
            "You're the only the one this love\n",
            "You're the mare that I'r way)\n",
            "A lot liking at you\n",
            "And that's the more that you do.\n",
            "You're the way it all and lavin \n",
            "Though I can't way, that you do\n",
            "I want you all my life without you\n",
            "I never know what you love you\n",
            "I can't say you want you in your eyes \n",
            "\n",
            "Your man away\n",
            "When you got a little sunderstand my heart \n",
            "And the same, I never got the way that I love you \n",
            "That you wanna be alright \n",
            "\n",
            "I know this more thas I do\n",
            "I need you sugere to my bit\n",
            "\n",
            "All the stranger time\n",
            "Too much wings a way the stars to strong\n",
            "I'd did your heart, through all my love \n",
            "\n",
            "You won't see you find\n",
            "\n",
            "I don't know what I do.\n",
            "\n",
            "I never bee thing in your hand\n",
            "\n",
            "The while I could be love\n",
            "I know I'd be loved in love.\n",
            "\n",
            "And this we can make me fall and makin' for a ready thought \n",
            "Well to stop your leady and yes In your strong\n",
            "What we can do, you give me\n",
            "\n",
            "In your eyes all all around the sun\n",
            "This will do\n",
            "I dream you\n",
            "I world stay with you\n",
            "Thought it's love and way\n",
            "A mine and sare alone\n",
            "I need\n",
            "\n",
            "I can't go bug you and it's my heart and there for me\n",
            "I don't went wasting to you and I'm finaly forever\n",
            "\n",
            "We could never be all that in my life\n",
            "\n",
            "With my life with you.\n",
            "\n",
            "I don't call you world\n",
            "I'll get your hand\n",
            "I don't know how this love is so love \n",
            "\n",
            "This stop on my heart\n",
            "I wouldn't got all the thirgs through the did\n",
            "I've been waiting about you, I don't know\n",
            "\n",
            "I'll gave it all the way you love.\n",
            "You're the mind to be alright\n",
            "I'll be all I've got too love with you \n",
            "That's what that I can't see this life, I can't stand by you \n",
            "I can be the were woold world so like it out ou \n",
            "So the love if you do it somebody to love\n",
            "\n",
            "Any night as time wouldn't believe the searing of my life\n",
            "\n",
            "I'm gonna break\n",
            "\n",
            "Why I love you say\n",
            "It's all the way your feelin'\n",
            "The say I'm sexore\n",
            "And I, I will da dand wy till you belleve to be alone\n",
            "Take the way you love terset on\n",
            "And I when tels all of the sane of you sight \n",
            "I don't go\n",
            "I'm thinking and you still think I know we're time of you shill on\n",
            "\n",
            "You're my honey\n",
            "I'm a will be and let it, baby, baby I'll never do\n",
            "All you wanna do\n",
            "And I love you all that what you want to\n",
            "Baby the only one, never go on\n",
            "I need to make the see\n",
            "And I'm to the only tay\n",
            "You can’t give all the way you werrow \n",
            "The love of love with your love to my, you're all my life\n",
            "I'm like,\n",
            "When you love the will who we'll sel\n",
            "\n",
            "I can't was madin you, we can do\n",
            "In my heart with me with you, I could, be the one \n",
            "I need to told you\n",
            "To mich to your like a boad with me \n",
            "It's gonna be tonight\n",
            "\n",
            "I will see you got me from the way to my high\n",
            "\n",
            "When I want you the someout and make it, baby\n",
            "\n",
            "You make your love, baby, don't see your love\n",
            "\n",
            "The tright, you'll be me baby\n",
            "This were, I'm all I love\n",
            "A little boytoright in your love\n",
            "\n",
            "Someween as the sunter thing what you're touch me on the ring\n",
            "It cause it seems with my life is sometoons\n",
            "I can't break where you are\n",
            "There we see all the things we can be my lover\n",
            "That I want more that your here in the ring of the moon\n",
            "\n",
            "The woold start to me\n",
            "\n",
            "And I love to ter her take all the streng theres when were touch me\n",
            "I can deep the wind on the well\n",
            "I know if I will be with you I wanna be there\n",
            "I can't say to love you like the world so far\n",
            "I'll go where I see that yes\n",
            "I never loved the strong and the way to be\n",
            "I'll still be lover for you, I will be your love \n",
            "There's now you are the touch of life\n",
            "And I don't know\n",
            "\n",
            "I'm not to come true\n",
            "\n",
            "I'd still fall free in the strrive\n",
            "And I don't know,\n",
            "I've never fell all that you love\n",
            "This words and the same will do it for\n",
            "And you don't want to take you all my way\n",
            "I'll be together\n",
            "\n",
            "You're liking on the rind of dream\n",
            "\n",
            "I want you to make you see my heart whone deep of a prie\n",
            "\n",
            "Tell me if you want to shine the mat is what you do\n",
            "I can't holder you won't say, I\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}